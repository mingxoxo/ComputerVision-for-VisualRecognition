
## 지역불변특징량 기반 물체인식
지역 불변한 특징량 SIFT를 이용해 물체인식.<br>
### 전체코드
```
//1주차-실습1번

#include<opencv2/core/core.hpp>
#include<opencv2/highgui/highgui.hpp>
#include<opencv2/features2d/features2d.hpp>
#include<opencv2/nonfree/features2d.hpp>
#include<opencv2/calib3d/calib3d.hpp>
#include<opencv2/flann/flann.hpp>
#include <iostream>
#include <stdio.h>
#define RED Scalar(0,0,255)

using namespace cv;
Mat boxFindImg(std::vector<DMatch> good_match, std::vector<KeyPoint> img1keypoint, std::vector<KeyPoint> camkeypoint, Mat img1, Mat finalOutputImg);

int main()
{
	//검출 -> 기술 -> 매칭
	//*****************각자 검출하고싶은 물체의 이미지 파일 위치를 대입
	Mat cam, img1;
	img1 = imread("이미지 주소를 넣으세요", IMREAD_GRAYSCALE);
	//카메라 준비
	VideoCapture cap;
	cap.open(0); //0번째 카메라 open

	if (!cap.isOpened())
	{
		std::cout << "카메라 작동 불가" << std::endl;
		return -1;
	}

	if (!(img1.data))
	{
		printf("이미지를 로드할 수 없습니다.");
		return 0;
	}
	//결과나 진행상황을 보여줄 창 미리 생성
	namedWindow("camera", WINDOW_AUTOSIZE);
	namedWindow("img1의 키포인트");
	namedWindow("img2의 키포인트");
	namedWindow("매칭 결과");

	SIFT instance_FeatureDetector;//검출을 위한 인스턴스 생성
	std::vector<KeyPoint> img1keypoint, camkeypoint;
	instance_FeatureDetector.detect(img1, img1keypoint);//img1에 특징점을 img1keypoint에 저장

	for (;;) {
		cap.read(cam); 
		imshow("camera", cam);

		//esc키를 누르면 반복 종료
		int c = -1;
		c = waitKey(33);
		if (c == 27)//esc키를 누르면 반복 종료
			break;
		else if (c < 0)//esc를 제외한 아무키나 누르면 됨
			continue;

		//*****************카메라 이미지 준비 완료 1.검출 with sift (SiftFeatureDetector)
		instance_FeatureDetector.detect(cam, camkeypoint); //camera에서 읽어온 이미지에서의 특징점을 camkeypoint에 저장

			//이미지에서 키포인트를 검출했으므로 키포인트를 시각적으로 보여줌
		Mat displayOfImg1;
		drawKeypoints(img1, img1keypoint, displayOfImg1, Scalar(0, 0, 255), DrawMatchesFlags::DRAW_RICH_KEYPOINTS);//빨간색 DRAW_RICH_KEYPOINTS로 나타냄
		imshow("img1의 키포인트", displayOfImg1);

		Mat displayOfImg2;
		drawKeypoints(cam, camkeypoint, displayOfImg2, Scalar(0, 0, 255), DrawMatchesFlags::DRAW_RICH_KEYPOINTS);//빨간색 DRAW_RICH_KEYPOINTS로 나타냄
		imshow("img2의 키포인트", displayOfImg2);

		//*****************1.기술 with sift (SiftDescriptorExtractor)
		SIFT instance_Descriptor;
		Mat img1outputarray, camoutputarray;
		instance_Descriptor.compute(img1, img1keypoint, img1outputarray);
		instance_Descriptor.compute(cam, camkeypoint, camoutputarray);

		std::cout << "기술 완료" << std::endl;

		//*****************1.매칭 with sift (SiftDescriptorExtractor)
		FlannBasedMatcher FLANNmatcher;
		std::vector<DMatch> match;
		FLANNmatcher.match(img1outputarray, camoutputarray, match);

		if (!(match.size()))
		{
			std::cout << "키포인트 매칭 불가!" << std::endl;
			return -1;
		}

		//매칭된 쌍들 중에서 유클리드 거리를 기준으로 굿매치(믿을만한 매칭)를 추출
		double maxd = 0; double mind = match[0].distance;
		for (int i = 0; i < match.size(); i++)
		{
			double dist = match[i].distance;
			if (dist < mind) mind = dist;
			if (dist > maxd) maxd = dist;
		}
		std::vector<DMatch> good_match;
		for (int i = 0; i < match.size(); i++)
			if (match[i].distance <= max(2 * mind, 0.02)) good_match.push_back(match[i]);

		Mat finalOutputImg;
		std::cout << "match 의 갯수는: " << match.size() << std::endl;
		std::cout << "good match 의 갯수는: " << good_match.size() << std::endl;

		drawMatches(img1, img1keypoint, cam, camkeypoint, good_match, finalOutputImg, Scalar(150, 30, 200), Scalar(0, 0, 255), std::vector< char >(), DrawMatchesFlags::DEFAULT);
		finalOutputImg = boxFindImg(good_match, img1keypoint, camkeypoint, img1, finalOutputImg);
		imshow("매칭 결과", finalOutputImg);
	}

	return 0;
}
Mat boxFindImg(std::vector<DMatch> good_match, std::vector<KeyPoint> img1keypoint, std::vector<KeyPoint> camkeypoint, Mat img1, Mat finalOutputImg)
{
	std::vector<Point2f> model_pt;
	std::vector<Point2f> scene_pt;
	for (int i = 0; i < good_match.size(); i++) {
		model_pt.push_back(img1keypoint[good_match[i].queryIdx].pt);
		scene_pt.push_back(camkeypoint[good_match[i].trainIdx].pt);
	}
	Mat H = findHomography(model_pt, scene_pt, CV_RANSAC);

	std::vector<Point2f> model_corner(4);
	model_corner[0] = cvPoint(0, 0);
	model_corner[1] = cvPoint(img1.cols, 0);
	model_corner[2] = cvPoint(img1.cols, img1.rows);
	model_corner[3] = cvPoint(0, img1.rows);

	std::vector<Point2f> scene_corner(4);
	perspectiveTransform(model_corner, scene_corner, H);
	//좌표에 투영변환 H를 적용하여 장면 영상에 나타날 좌표 scene_corner를 계산한다

	Point2f p(img1.cols, 0); //4개의 선분영상 그려넣음으로써 인식 결과 표시
							 //하나의 윈도우에 모델 영상과 장면 영상을 같이 그려넣었기때문에 모델 영상의 너비만큼 이동시켜 그리기 위해 
							 //Point2f 형의 p를 더해주었다.
	line(finalOutputImg, scene_corner[0] + p, scene_corner[1] + p, RED, 3);
	line(finalOutputImg, scene_corner[1] + p, scene_corner[2] + p, RED, 3);
	line(finalOutputImg, scene_corner[2] + p, scene_corner[3] + p, RED, 3);
	line(finalOutputImg, scene_corner[3] + p, scene_corner[0] + p, RED, 3);

	return finalOutputImg;
}
```

## 기능별 설명

# 카메라 열어서 영상 읽어오기

## 코드에 사용될 개념
**Mat**
[mat 튜도리얼 링크](https://docs.opencv.org/2.4.13.2/doc/tutorials/core/mat_the_basic_image_container/mat_the_basic_image_container.html?highlight=mat)
mat은 opencv에서 이미지를 저장할 수 있는 class 입니다. 
![image](https://user-images.githubusercontent.com/41140561/52396381-12aa1000-2af5-11e9-8bf8-9b51a558c2a6.png)
컴퓨터는 이미지를 숫자로 읽습니다.
이러한 숫자를 행렬 구조체인 mat에 저장하여 이미지를 저장할 수 있습니다.
<br>
**[videoCapture](https://docs.opencv.org/2.4.13.2/modules/highgui/doc/reading_and_writing_images_and_video.html?highlight=videocapture#videocapture)**
VideoCapture은 비디오파일이나 카메라에서 받아온 연속적인 이미지에서 캡처를 수행하거나, 그러한 연속적인 이미지를 읽어오는 C++ API를 제공하는 Class입니다.
C++에서
```
C++: bool VideoCapture::open(const string& filename)
C++: bool VideoCapture::open(int device)
```
위를 통해 filename에 저장된 비디오 또는 연결된 카메라를 오픈할 수 있습니다.
` VideoCapture::release()` 를 이용해 연 파일이나 카메라를 닫을 수 있습니다.
또한 `C++: bool VideoCapture::isOpened()`를 통해 파일이나 카메라가 열렸는지 확인할 수 있습니다.
open으로 오픈 후 위의 api를 이용해 정상적으로 열렸는지 확인할 때 사용됩니다.
`C++: bool VideoCapture::read(Mat& image)`는 video파일을 읽거나 데이터를 캡처하고 그 이미지를 반환합니다.
<br>
**[namedWindow](https://docs.opencv.org/2.4.13.2/modules/highgui/doc/user_interface.html?highlight=namedwindow#namedwindow)**
`C++: void namedWindow(const string& winname, int flags=WINDOW_AUTOSIZE )` 윈도우를 생성합니다. flag값을 통해 윈도우의 크기를 정합니다.
```
//flag
WINDOW_NORMAL로 지정하면 사용자가 resize할 수 있습니다.
WINDOW_AUTOSIZE로 지정하면 이미지에 맞게 자동적으로 window의 크기가 지정됩니다.
```
<br>

**[imshow](https://docs.opencv.org/2.4.13.2/modules/highgui/doc/user_interface.html?highlight=namedwindow#imshow)**
`C++: void imshow(const string& winname, InputArray mat)` 특정 윈도우(winname)에 이미지(mat)을 display합니다. 윈도우가 없다면 윈도우를 생성합니다.
<br>
**imread**
`C++: Mat imread(const string& filename, int flags=1 )`
filename에서 이미지를 가져온다.
flags로 가져오는 이미지의 타입을 설정할 수 있는데 종류는 아래와 같다.
```
CV_LOAD_IMAGE_ANYDEPTH - If set, return 16-bit/32-bit image when the input has the corresponding depth, otherwise convert it to 8-bit.
CV_LOAD_IMAGE_COLOR - If set, always convert image to the color one
CV_LOAD_IMAGE_GRAYSCALE - If set, always convert image to the grayscale one
>0 Return a 3-channel color image.
=0 Return a grayscale image.
<0 Return the loaded image as is (with alpha channel).
```

## 코드
```
#include <opencv2/opencv.hpp>
#include<iostream>

int main(int argc, char**argv)
{
	cv::namedWindow("camera", cv::WINDOW_AUTOSIZE);
	cv::VideoCapture capture(0);

	if (!capture.isOpened) 
	{
		std::cout << "카메라가 열리지 않았습니다." << std::endl;
		return 1;
	}

	cv::Mat bgr_frame;
	for (;;)
	{
		capture.read(bgr_frame);
		if (bgr_frame.empty()) break;

		cv::imshow("camera", bgr_frame);

		char c = cv::waitKey(10);
		if (c == 27) break;
	}

	capture.release();
	return 0;
}

```

# 이미지에서 SIFT로 키포인트 추출해 매칭시키기

## 코드에 사용될 개념
**[SIFT](https://docs.opencv.org/2.4.13.2/modules/nonfree/doc/feature_detection.html#sift)**
SIFT는 Scale Invariant Feature Transform (SIFT) 알고리즘을 이용해 keypoint를 찾고, 기술자를 기술하는 클래스이다.
SIFT는 Feature2D에서 상속된다. 따라서 Feature2D에 있는 api도 이용할 수 있다.

`C++: void FeatureDetector::detect(const Mat& image, vector<KeyPoint>& keypoints, const Mat& mask=Mat() ) ` [링크](https://docs.opencv.org/2.4.13.2/modules/features2d/doc/common_interfaces_of_feature_detectors.html#featuredetector-detect)<br>
이미지(Mat)로 부터 keypoint를 추출해 keypoint vector에 저장한다.

`C++: void DescriptorExtractor::compute(const Mat& image, vector<KeyPoint>& keypoints, Mat& descriptors)`[링크](https://docs.opencv.org/2.4.13.2/modules/features2d/doc/common_interfaces_of_descriptor_extractors.html#void%20DescriptorExtractor::compute(const%20Mat&%20image,%20vector%3CKeyPoint%3E&%20keypoints,%20Mat&%20descriptors)%20const) <br>
이미지(image)에서 감지된 ketpoint세트(keypoints)에서 기술자를 추출하고 outputMat(descriptors)에 기술자를 저장한다.
<br>
**[FlannBasedMatcher](https://docs.opencv.org/2.4/modules/features2d/doc/common_interfaces_of_descriptor_matchers.html?highlight=flannbasedmatcher#flannbasedmatcher)**
Flann은 Fast Library for Approximate Nearest Neighbors의 줄임말로 대략 근접이웃을 통해 빠른 매칭을 할 수 있다.
Flann기반 matcher를 이용할 수 있는 class이다 DescriptorMatcher에서 상속되었다.
`C++: void DescriptorMatcher::match(const Mat& queryDescriptors, const Mat& trainDescriptors, vector<DMatch>& matches, const Mat& mask=Mat() )`[링크](https://docs.opencv.org/2.4/modules/features2d/doc/common_interfaces_of_descriptor_matchers.html#descriptormatcher-match) <br>
두개의 기술자를 이용해 유사한 키포인트 쌍을 매칭해 Dmatch세트(matches)에 저장한다.
<br>
**[KeyPoint](https://docs.opencv.org/2.4/modules/features2d/doc/common_interfaces_of_feature_detectors.html?highlight=keypoint#keypoint)**
 한 점을 위한 데이터 구조체
<br>
**DMatch**
```
    int queryIdx; // query descriptor index
    int trainIdx; // train descriptor index
    int imgIdx;   // train image index

    float distance;
```
가 정의되어 있어, match된 keypoint쌍의 정보를 저장할 수 있다.
<br>
**[drawMatches,  drawKeypoints](https://docs.opencv.org/2.4/modules/features2d/doc/drawing_function_of_keypoints_and_matches.html#drawing-function-of-keypoints-and-matches)**

##### drawMatches
```
C++: void drawMatches(const Mat& img1, 
    const vector<KeyPoint>& keypoints1, 
    const Mat& img2, const vector<KeyPoint>& keypoints2, 
    const vector<DMatch>& matches1to2,
    Mat& outImg, 
    const Scalar& matchColor=Scalar::all(-1), 
    const Scalar& singlePointColor=Scalar::all(-1), 
    const vector<char>& matchesMask=vector<char>(), 
    int flags=DrawMatchesFlags::DEFAULT )
```
img1에서 추출한 keypoint세트 keypoints1와 img1, img2에서 추출한 keypoint세트 keypoints2와 img2,
두 keypoint의 매칭정보를 갖고있는 DMatch세트 matches1to2를 이용해 매칭 결과를 그려 outImg에 저장해 반환한다. 그 외의 인수들은 이름에서 알 수 있듯, 
matchColor는 매치(선과 연결된 keypoint)의 색, singlePointColor는 키포인트가 일치하지 않는 단일 키포인트(원)의 색, 
matchesMask는 어떤 매치를 그릴까 결정하는 마스크로, 이것이 empty값을 가지면 모든 matches를 그린다.
flags는 features들을 그리는 특징에 대한 설정값으로 DrawMatchesFlags에 정의되어 있고 아래와 같다.
```
struct DrawMatchesFlags
{
    enum
    {
        DEFAULT = 0, // Output image matrix will be created (Mat::create),
                     // i.e. existing memory of output image may be reused.
                     // Two source images, matches, and single keypoints
                     // will be drawn.
                     // For each keypoint, only the center point will be
                     // drawn (without a circle around the keypoint with the
                     // keypoint size and orientation).
        DRAW_OVER_OUTIMG = 1, // Output image matrix will not be
                       // created (using Mat::create). Matches will be drawn
                       // on existing content of output image.
        NOT_DRAW_SINGLE_POINTS = 2, // Single keypoints will not be drawn.
        DRAW_RICH_KEYPOINTS = 4 // For each keypoint, the circle around
                       // keypoint with keypoint size and orientation will
                       // be drawn.
    };
};
```
##### drawKeypoints
```
C++: void drawKeypoints(const Mat& image,
    const vector<KeyPoint>& keypoints,
    Mat& outImage,
    const Scalar& color=Scalar::all(-1),
    int flags=DrawMatchesFlags::DEFAULT
    )
```
매개 변수 :
image - 소스 이미지
keypoints - 소스 이미지에서의 키포인트
outImage - Output image. 그것의 내용은 출력 이미지에 무엇을 그리는지를 정의하는 flags 값에 따라 달라진다.
color - 키포인트의 색상
flags - 플래그 설정 도면 피쳐 가능한 플래그 비트 값은 DrawMatchFlags에 의해 정의된다. 위의 drawMatch()에서 자세한 내용을 참조한다.


## 코드
```
#include<opencv2/core/core.hpp>
#include<opencv2/highgui/highgui.hpp>
#include<opencv2/features2d/features2d.hpp>
#include<opencv2/nonfree/features2d.hpp>
#include<opencv2/calib3d/calib3d.hpp>
#include<opencv2/flann/flann.hpp>
#include<opencv2/imgproc/imgproc.hpp>
#include <iostream>
#include <stdio.h>

using namespace cv;

int main()
{
	//검출 -> 기술 -> 매칭
	//*****************각자 검출하고싶은 물체의 이미지 파일 위치를 대입
	Mat img1, img2;
	//연산속도를 높이기 위해 IMREAD_GRAYSCALE로 gray이미지로 읽어오고 사이즈를 줄인다.
	img1 = imread("이미지 주소를 넣으세요", IMREAD_GRAYSCALE);
	img2 = imread("이미지 주소를 넣으세요", IMREAD_GRAYSCALE);
	Size size(img1.cols / 2, img1.rows / 2);
	resize(img1, img1, size);
	resize(img2, img2, size);


	if (!(img1.data && img2.data))
	{
		printf("이미지를 로드할 수 없습니다.");
		return 0;
	}
	//결과나 진행상황을 보여줄 창 미리 생성
	namedWindow("img1의 키포인트");
	namedWindow("img2의 키포인트");
	namedWindow("매칭 결과");

	//*****************1.검출 with sift (SiftDescriptorExtractor)
	SIFT instance_FeatureDetector;//검출을 위한 인스턴스 생성
	std::vector<KeyPoint> img1keypoint, img2keypoint;
	instance_FeatureDetector.detect(img1, img1keypoint);//img1에 특징점을 img1keypoint에 저장
	instance_FeatureDetector.detect(img1, img2keypoint);//img1에 특징점을 img1keypoint에 저장
	//keypoint 검출 결과를 보이기
	Mat displayOfImg1, displayOfImg2;
	drawKeypoints(img1, img1keypoint, displayOfImg1, Scalar(0, 0, 255), DrawMatchesFlags::DRAW_RICH_KEYPOINTS);//빨간색 DRAW_RICH_KEYPOINTS로 나타냄
	drawKeypoints(img2, img2keypoint, displayOfImg2, Scalar(0, 0, 255), DrawMatchesFlags::DRAW_RICH_KEYPOINTS);//빨간색 DRAW_RICH_KEYPOINTS로 나타냄

	imshow("img1의 키포인트", displayOfImg1);
	imshow("img2의 키포인트", displayOfImg2);

	//*****************1.기술 with sift (SiftDescriptorExtractor)
	SIFT instance_Descriptor;
	Mat img1outputarray, img2outputarray;
	instance_Descriptor.compute(img1, img1keypoint, img1outputarray);
	instance_Descriptor.compute(img2, img2keypoint, img2outputarray);

	//*****************1.매칭 with sift (SiftDescriptorExtractor)
	FlannBasedMatcher FLANNmatcher;
	std::vector<DMatch> match;
	FLANNmatcher.match(img1outputarray, img2outputarray, match);
	if (!(match.size()))
	{
		std::cout << "키포인트 매칭 불가!" << std::endl;
		return -1;
	}

	//매칭된 쌍들 중에서 유클리드 거리를 기준으로 굿매치(믿을만한 매칭)을 추출
	double maxd = 0; double mind = match[0].distance;
	for (int i = 0; i < match.size(); i++)
	{
		double dist = match[i].distance;
		if (dist < mind) mind = dist;
		if (dist > maxd) maxd = dist;
	}
	std::vector<DMatch> good_match;
	for (int i = 0; i < match.size(); i++)
		if (match[i].distance <= max(2 * mind, 0.02)) good_match.push_back(match[i]);


	Mat finalOutputImg;
	std::cout << "match 의 갯수는: " << match.size() << std::endl;
	std::cout << "good match 의 갯수는: " << good_match.size() << std::endl;
	//good_match인 match쌍들을 보이기
	drawMatches(img1, img1keypoint, img2, img2keypoint, good_match, finalOutputImg, Scalar(150, 30, 200), Scalar(0, 0, 255), std::vector< char >(), DrawMatchesFlags::DEFAULT);
	imshow("매칭 결과", finalOutputImg);
	waitKey(0);
	return 0;
}
```

# 찾은 물체에 대한 표시를 위해 선으로 박스모양 만들기

## 코드에 사용될 개념
**std::vector**
벡터(std::vector)는 배열의 기능을 확장한 것
ex ) std::vector model_pt;
//Point2f형의 벡터 변수 model_pt 선언 및 초기화
model_pt.push_back(img1keypoint[good_match[i].queryIdx].pt);
변수 model_pt의 끝 값에 img1keypoint[good_match[i].queryIdx].pt를 추가해준다.
https://blog.naver.com/gnqo7598/221326944267

**findHomography()**
: 다수의 매칭쌍으로부터 homography 행렬을 계산해 줌(근사 방법은 전체 데이터 fitting, RANSAC, LMedS 중 선택),(opencv2 / calib3d / calib3d.hpp)
-Homograhpy
한 평면을 다른 평면에 투영(Projection) 시켰을 때, 투영된 대응점들 사이에서는 일정한 변환 관계가 성립하는데 이 변환 관계를 Homography라고 합니다.
https://darkpgmr.tistory.com/m/79

**cvPoint**
자료형 : CvPoint 멤버변수 : x,y(int형) 인라인 함수 이름 : cvPoint - int형 2D 화소 위치
https://blog.naver.com/gnqo7598/221326944267

**perspectiveTransform()**
: 3x3 homography 변환행렬 또는 4×4 변환행렬을 이용하여 좌표변환을 할 때 사용

**line**
line(선을 그릴 윈도우창, 첫번째 점, 두번째 점, 색깔, 선의 굵기, 선의 타입, 선의 이동)
선을 그려주는 역할

## 코드
```
Mat boxFindImg(std::vector<DMatch> good_match, std::vector<KeyPoint> img1keypoint, 
 std::vector<KeyPoint> camkeypoint, Mat img1, Mat finalOutputImg)
 //(이미지의 good_match, 이미지1의 키포인트, 캠에서의 키포인트, 이미지1, 결과를 저장할 Mat형 변수)
 {
	//벡터(std::vector)는 배열의 기능을 확장한 것 - <Point2f>형
	std::vector<Point2f> model_pt;
	std::vector<Point2f> scene_pt;
	for (int i = 0; i < good_match.size(); i++) { //good_match를 추가
		//push_back() -> 벡터 끝에 주어진 원소값을 추가한다
		model_pt.push_back(img1keypoint[good_match[i].queryIdx].pt);
		scene_pt.push_back(camkeypoint[good_match[i].trainIdx].pt);
	}
	Mat H = findHomography(model_pt, scene_pt, CV_RANSAC);
	
	std::vector<Point2f> model_corner(4);
	//cols -> 가로, rows -> 세로

	model_corner[0] = cvPoint(0, 0);
	model_corner[1] = cvPoint(img1.cols, 0);
	model_corner[2] = cvPoint(img1.cols, img1.rows);
	model_corner[3] = cvPoint(0, img1.rows);

	std::vector<Point2f> scene_corner(4);
	perspectiveTransform(model_corner, scene_corner, H);
	
	//좌표에 투영변환 H를 적용하여 장면 영상에 나타날 좌표 scene_corner를 계산한다

	Point2f p(img1.cols, 0); //4개의 선분영상 그려넣음으로써 인식 결과 표시
							 //하나의 윈도우에 모델 영상과 장면 영상을 같이 그려넣었기때문에 
 모델 영상의 너비만큼 이동시켜 그리기 위해 
							 //Point2f 형의 p를 더해주었다.
	//line (선을 그릴 Mat형 변수 , 첫번째 점, 두번째 점, 색깔, 선의 굵기, 선의 타입, 선의 이동)
	line(finalOutputImg, scene_corner[0] + p, scene_corner[1] + p, RED, 3, 14, 0);
	line(finalOutputImg, scene_corner[1] + p, scene_corner[2] + p, BLUE, 3);
	line(finalOutputImg, scene_corner[2] + p, scene_corner[3] + p, GREEN, 3);
	line(finalOutputImg, scene_corner[3] + p, scene_corner[0] + p, Scalar(255, 255, 255), 3);

	return finalOutputImg;
 }
```
